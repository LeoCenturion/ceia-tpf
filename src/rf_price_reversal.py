import pandas as pd
import numpy as np
from scipy.stats import pearsonr
from scipy.signal import find_peaks
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler

from backtest_utils import fetch_historical_data

# Check if pandas_ta is installed
try:
    import pandas_ta as ta
except ImportError:
    ta = None


def create_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    Creates a comprehensive set of technical analysis features using pandas_ta.
    """
    # Use pandas_ta to generate a large number of indicators
    df.ta.strategy("all", append=True)
    
    # Clean up column names generated by pandas_ta for compatibility
    df.columns = df.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True)
    
    # Fill NaN values that might have been generated
    df.bfill(inplace=True)
    df.ffill(inplace=True)
    
    return df

def create_target_variable(df: pd.DataFrame) -> pd.DataFrame:
    """
    Identifies local tops (1) and bottoms (0) using the Awesome Oscillator (AO)
    and returns a DataFrame containing only the rows with these reversal points.
    """
    # Calculate Awesome Oscillator
    ao = ta.ao(df['High'], df['Low'])
    if ao is None or ao.isnull().all():
        # Return an empty DataFrame if AO can't be calculated
        return pd.DataFrame(columns=df.columns.tolist() + ['target'])
    
    # Find peaks (tops) and troughs (bottoms) in the AO
    peaks, _ = find_peaks(ao)
    troughs, _ = find_peaks(-ao)
    
    # Create the target column, initially with NaNs
    df['target'] = np.nan
    df.loc[df.index[peaks], 'target'] = 1  # Tops
    df.loc[df.index[troughs], 'target'] = 0 # Bottoms
    
    # We are only interested in classifying the reversal points
    reversal_points_df = df.dropna(subset=['target'])
    
    return reversal_points_df

def select_features(X: pd.DataFrame, y: pd.Series, corr_threshold=0.7, p_value_threshold=0.05) -> list:
    """
    Selects features based on Pearson correlation and p-value.
    Note: The correlation threshold of 0.7 is extremely high and might result in
    very few or no features being selected. The paper's methodology is followed here.
    """
    selected_features = []
    for col in X.columns:
        # Drop rows with NaN in either column for correlation calculation
        temp_df = pd.concat([X[col], y], axis=1).dropna()
        if len(temp_df) < 2:
            continue

        corr, p_value = pearsonr(temp_df.iloc[:, 0], temp_df.iloc[:, 1])
        
        if abs(corr) >= corr_threshold and p_value < p_value_threshold:
            selected_features.append(col)
            
    print(f"Selected {len(selected_features)} features out of {len(X.columns)} based on correlation criteria.")
    return selected_features

def main():
    """
    Main function to run the Random Forest price reversal classification task.
    """
    if not ta:
        print("This script requires the 'pandas_ta' library. Please install it using 'pip install pandas_ta'")
        return

    # 1. Load Data
    print("Loading historical data...")
    data = fetch_historical_data(
        data_path="/home/leocenturion/Documents/postgrados/ia/tp-final/Tp Final/data/BTCUSDT_1h.csv",
        start_date="2022-01-01T00:00:00Z"
    )
    
    # 2. Create Target Variable
    print("Identifying tops and bottoms to create target variable...")
    reversal_data = create_target_variable(data.copy())
    
    if reversal_data.empty:
        print("No reversal points (tops/bottoms) were identified. Exiting.")
        return
        
    y = reversal_data['target']
    
    # 3. Create Features for the entire dataset
    print("Generating technical analysis features...")
    features_df = create_features(data.copy())
    
    # Align features with the reversal points
    X = features_df.loc[reversal_data.index]
    
    # Drop original OHLCV and the target from the feature set
    # Also drop any columns that might be constant
    X = X.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume', 'target'], errors='ignore')
    X = X.loc[:, (X != X.iloc[0]).any()]

    # 4. Feature Selection
    print("Performing feature selection...")
    selected_cols = select_features(X, y)
    
    if not selected_cols:
        print("No features met the high correlation criteria. Using a default set of common indicators instead.")
        # Fallback to a few common indicators if selection yields nothing.
        default_features = pd.DataFrame(index=data.index)
        default_features['RSI'] = ta.rsi(data.Close)
        default_features['MACD'] = ta.macd(data.Close)['MACDh_12_26_9']
        default_features['MFI'] = ta.mfi(data.High, data.Low, data.Close, data.Volume)
        X = default_features.loc[reversal_data.index]
        X.bfill(inplace=True)
        X.ffill(inplace=True)
    else:
        X = X[selected_cols]
        
    print(f"Final features being used: {list(X.columns)}")

    # 5. Split Data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    # 6. Scale Features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # 7. Train Model
    print("Training Random Forest Classifier...")
    model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    model.fit(X_train_scaled, y_train)
    
    # 8. Evaluate Model
    print("Evaluating model performance...")
    y_pred = model.predict(X_test_scaled)
    
    # Print classification report
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=['Bottom (0)', 'Top (1)'], zero_division=0))
    
    # Print overall accuracy
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Overall Accuracy: {accuracy:.2f}")

if __name__ == "__main__":
    main()
