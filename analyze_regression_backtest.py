#%%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Configuration
RESULTS_FILE = 'regression_backtest_results.csv'

# Load the backtest results
try:
    results_df = pd.read_csv(RESULTS_FILE, index_col='timestamp', parse_dates=True)
    print(f"Successfully loaded data from {RESULTS_FILE}. Shape: {results_df.shape}")
except FileNotFoundError:
    print(f"Error: The file '{RESULTS_FILE}' was not found. Please ensure it has been generated by 'run_regression_backtest()'.")
    exit()

# Calculate error metrics
results_df['error'] = results_df['actual_close'] - results_df['predicted_close']
results_df['absolute_error'] = np.abs(results_df['error'])
results_df['percentage_error'] = (results_df['error'] / results_df['actual_close']) * 100
results_df['absolute_percentage_error'] = np.abs(results_df['percentage_error'])

print("\nError Statistics:")
print(results_df[['error', 'absolute_error', 'percentage_error', 'absolute_percentage_error']].describe())

#%%
# --- Comparison of multiple backtest results ---

# Add more file paths to this list to compare different model results
# For example: 'xgboost_regression_backtest_results.csv'
RESULTS_FILES = [
    'regression_backtest_results.csv',
    'regression_backtest_results_7d.csv',
    'regression_backtest_results_14d.csv',
    'regression_backtest_results_365d.csv'
]

comparison_metrics = []
#AI compare in the barchart the min, max, mean, and percentiles from doing results_df[['error', 'absolute_error', 'percentage_error', 'absolute_percentage_error']].describe() for each file AI!  
for file in RESULTS_FILES:
    try:
        df = pd.read_csv(file, index_col='timestamp', parse_dates=True)
        # Create a more readable model name from the filename
        model_name = file.replace('_backtest_results.csv', '').replace('regression_', '')

        # Calculate error metrics
        df['error'] = df['actual_close'] - df['predicted_close']
        df['absolute_error'] = np.abs(df['error'])
        df['percentage_error'] = (df['error'] / df['actual_close']) * 100
        df['absolute_percentage_error'] = np.abs(df['percentage_error'])

        # Calculate summary statistics for comparison
        mae = df['absolute_error'].mean()
        mape = df['absolute_percentage_error'].mean()
        rmse = np.sqrt((df['error'] ** 2).mean())

        comparison_metrics.append({
            'model': model_name,
            'Mean Absolute Error (MAE)': mae,
            'Mean Absolute Percentage Error (MAPE)': mape,
            'Root Mean Squared Error (RMSE)': rmse,
        })
    except FileNotFoundError:
        print(f"\nWarning: The file '{file}' was not found. Skipping comparison for this file.")

if comparison_metrics:
    comparison_df = pd.DataFrame(comparison_metrics).set_index('model')
    print("\n--- Model Comparison ---")
    print(comparison_df)

    # Plotting the comparison
    comparison_df.plot(kind='bar', figsize=(14, 8), subplots=True, layout=(1, 3), sharey=False, rot=0)
    plt.suptitle('Comparison of Model Performance Metrics', fontsize=16)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()

#%%
# --- Detailed analysis of primary file continues below ---
print(f"\n--- Detailed Analysis for: {RESULTS_FILE} ---")

# 1. Analyze the distribution of the error
plt.figure(figsize=(15, 6))

# Histogram
plt.subplot(1, 2, 1)
sns.histplot(results_df['error'], kde=True, bins=50, color='skyblue')
plt.title('Distribution of Prediction Error')
plt.xlabel('Error (Actual - Predicted)')
plt.ylabel('Frequency')
plt.grid(True, linestyle='--', alpha=0.6)

# KDE Plot
plt.subplot(1, 2, 2)
sns.kdeplot(results_df['error'], fill=True, color='lightcoral')
plt.title('Kernel Density Estimate of Prediction Error')
plt.xlabel('Error (Actual - Predicted)')
plt.ylabel('Density')
plt.grid(True, linestyle='--', alpha=0.6)

plt.tight_layout()
plt.show()

#%%
plt.figure(figsize=(15, 6))
# Histogram
plt.subplot(1, 2, 1)
sns.histplot(results_df['percentage_error'], kde=True, bins=50, color='skyblue')
plt.title('Distribution of Prediction Error')
plt.xlabel('Error (Actual - Predicted)')
plt.ylabel('Frequency')
plt.grid(True, linestyle='--', alpha=0.6)

# KDE Plot
plt.subplot(1, 2, 2)
sns.kdeplot(results_df['percentage_error'], fill=True, color='lightcoral')
plt.title('Kernel Density Estimate of Prediction Error')
plt.xlabel('Error (Actual - Predicted)')
plt.ylabel('Density')
plt.grid(True, linestyle='--', alpha=0.6)

plt.tight_layout()
plt.show()

#%%
plt.figure(figsize=(15, 6))
# Histogram
plt.subplot(1, 2, 1)
sns.histplot(results_df['absolute_percentage_error'], kde=True, bins=50, color='skyblue')
plt.title('Distribution of Prediction Error')
plt.xlabel('Error (Actual - Predicted)')
plt.ylabel('Frequency')
plt.grid(True, linestyle='--', alpha=0.6)

# KDE Plot
plt.subplot(1, 2, 2)
sns.kdeplot(results_df['absolute_percentage_error'], fill=True, color='lightcoral')
plt.title('Kernel Density Estimate of Prediction Error')
plt.xlabel('Error (Actual - Predicted)')
plt.ylabel('Density')
plt.grid(True, linestyle='--', alpha=0.6)

plt.tight_layout()
plt.show()

#%%
# 2. Plot the relationship between the absolute value of the error and the amount of periods since last refit
# Convert periods_since_refit to days (assuming hourly data, 24 periods per day)
results_df['days_since_refit'] = results_df['periods_since_refit'] / 24
results_df['days_since_refit'] = results_df['days_since_refit'].astype(int)
plt.figure(figsize=(12, 7))
sns.boxplot(
    data=results_df,
    x='days_since_refit',
    y='absolute_percentage_error',
)

plt.title('Percentage Absolute Error vs. Days Since Last Model Refit')
plt.xlabel('Days Since Last Refit')
plt.ylabel('% Absolute Error |Actual - Predicted| / Actual')
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend(title='Days Since Refit')
plt.tight_layout()
plt.show()
#%%
# 3. Plot the error through time
plt.figure(figsize=(15, 7))
plt.plot(results_df.index, results_df['error'], label='Prediction Error', color='darkorange', alpha=0.8)
plt.axhline(0, color='gray', linestyle='--', linewidth=1) # Zero error line
plt.title('Prediction Error Through Time')
plt.xlabel('Date')
plt.ylabel('Error (Actual - Predicted)')
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend()
plt.tight_layout()
plt.show()

#%%
# Optional: Plot absolute percentage error through time
plt.figure(figsize=(15, 7))
plt.plot(results_df.index, results_df['absolute_percentage_error'], label='Absolute Percentage Error', color='purple', alpha=0.8)
plt.title('Absolute Percentage Error Through Time')
plt.xlabel('Date')
plt.ylabel('Absolute Percentage Error (%)')
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend()
plt.tight_layout()
plt.show()

