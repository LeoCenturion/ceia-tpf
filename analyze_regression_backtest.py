#%%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Configuration
RESULTS_FILE = 'regression_backtest_results.csv'
# Load the backtest results
try:
    results_df = pd.read_csv(RESULTS_FILE, index_col='timestamp', parse_dates=True)
    print(f"Successfully loaded data from {RESULTS_FILE}. Shape: {results_df.shape}")
except FileNotFoundError:
    print(f"Error: The file '{RESULTS_FILE}' was not found. Please ensure it has been generated by 'run_regression_backtest()'.")
    exit()

# Calculate error metrics
results_df['error'] = results_df['actual_close'] - results_df['mean']
results_df['absolute_error'] = np.abs(results_df['error'])
results_df['percentage_error'] = (results_df['error'] / results_df['actual_close']) * 100
results_df['absolute_percentage_error'] = np.abs(results_df['percentage_error'])

print("\nError Statistics:")
print(results_df[['error', 'absolute_error', 'percentage_error', 'absolute_percentage_error']].describe())

#%%
# --- Comparison of multiple backtest results ---


RESULTS_FILES = [
    'regression_backtest_results.csv',
    'regression_backtest_results_7d.csv',
    'regression_backtest_results_14d.csv',
    'regression_backtest_results_365d.csv'
]

all_stats_list = []
summary_metrics = []
for file in RESULTS_FILES:
    try:
        df = pd.read_csv(file, index_col='timestamp', parse_dates=True)
        # Create a more readable model name from the filename
        model_name = file.replace('_backtest_results.csv', '').replace('regression_', '')

        # Calculate error metrics
        df['error'] = df['actual_close'] - df['mean']
        df['absolute_error'] = np.abs(df['error'])
        df['percentage_error'] = (df['error'] / df['actual_close']) * 100
        df['absolute_percentage_error'] = np.abs(df['percentage_error'])

        # Get summary statistics
        stats = df[['error', 'absolute_error', 'percentage_error', 'absolute_percentage_error']].describe()
        stats['model'] = model_name
        all_stats_list.append(stats)

        # Calculate summary metrics like MAE, RMSE, MASE
        mae = df['absolute_error'].mean()
        mape = df['absolute_percentage_error'].mean()
        rmse = np.sqrt((df['error']**2).mean())
        mae_naive = np.abs(df['actual_close'].diff()).mean()
        mase = mae / mae_naive if mae_naive != 0 else np.inf

        summary_metrics.append({
            'model': model_name,
            'MAE': mae,
            'MAPE': mape,
            'RMSE': rmse,
            'MASE': mase,
        })
        
    except FileNotFoundError:
        print(f"\nWarning: The file '{file}' was not found. Skipping comparison for this file.")

if all_stats_list:
    # Prepare dataframe for plotting
    comparison_df = pd.concat(all_stats_list)
    comparison_df = comparison_df.reset_index().rename(columns={'index': 'statistic'})
    comparison_df = comparison_df.set_index(['model', 'statistic'])

    print("\n--- Model Comparison Statistics ---")
    print(comparison_df)

    # Plotting
    metrics_to_plot = ['error', 'absolute_error', 'percentage_error', 'absolute_percentage_error']
    stats_to_plot = ['mean', 'std', 'min', '25%', '50%', '75%', 'max']

    for metric in metrics_to_plot:
        # Select data for the current metric and stats to plot
        # Using .loc to avoid potential KeyError if a stat is missing, though describe() is consistent
        plot_data = comparison_df[metric].unstack(level='model').loc[stats_to_plot]

        # Plotting the comparison
        plot_data.plot(kind='bar', figsize=(14, 8), rot=0, width=0.8)
        plt.title(f'Comparison of Statistics for: {metric.replace("_", " ").title()}', fontsize=16)
        plt.xlabel('Statistic')
        plt.ylabel('Value')
        plt.grid(axis='y', linestyle='--', alpha=0.7)
        plt.legend(title='Model')
        plt.tight_layout()
        plt.show()

if summary_metrics:
    summary_df = pd.DataFrame(summary_metrics).set_index('model')
    print("\n--- Model Comparison (Summary Metrics) ---")
    print(summary_df)

    # Plotting
    summary_df.plot(kind='bar', figsize=(12, 8), subplots=True, layout=(2, 2), sharey=False, rot=45)
    plt.suptitle('Comparison of Summary Error Metrics', fontsize=16)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()
#%%
# --- Detailed analysis of primary file continues below ---
print(f"\n--- Detailed Analysis for: {RESULTS_FILE} ---")

# 1. Analyze the distribution of the error
plt.figure(figsize=(15, 6))

# Histogram
plt.subplot(1, 2, 1)
sns.histplot(results_df['error'], kde=True, bins=50, color='skyblue')
plt.title('Distribution of Prediction Error')
plt.xlabel('Error (Actual - Mean)')
plt.ylabel('Frequency')
plt.grid(True, linestyle='--', alpha=0.6)

# KDE Plot
plt.subplot(1, 2, 2)
sns.kdeplot(results_df['error'], fill=True, color='lightcoral')
plt.title('Kernel Density Estimate of Prediction Error')
plt.xlabel('Error (Actual - Mean)')
plt.ylabel('Density')
plt.grid(True, linestyle='--', alpha=0.6)

plt.tight_layout()
plt.show()

#%%
plt.figure(figsize=(15, 6))
# Histogram
plt.subplot(1, 2, 1)
sns.histplot(results_df['percentage_error'], kde=True, bins=50, color='skyblue')
plt.title('Distribution of Prediction Error')
plt.xlabel('Error (Actual - Mean)')
plt.ylabel('Frequency')
plt.grid(True, linestyle='--', alpha=0.6)

# KDE Plot
plt.subplot(1, 2, 2)
sns.kdeplot(results_df['percentage_error'], fill=True, color='lightcoral')
plt.title('Kernel Density Estimate of Prediction Error')
plt.xlabel('Error (Actual - Mean)')
plt.ylabel('Density')
plt.grid(True, linestyle='--', alpha=0.6)

plt.tight_layout()
plt.show()

#%%
plt.figure(figsize=(15, 6))
# Histogram
plt.subplot(1, 2, 1)
sns.histplot(results_df['absolute_percentage_error'], kde=True, bins=50, color='skyblue')
plt.title('Distribution of Prediction Error')
plt.xlabel('Error (Actual - Mean)')
plt.ylabel('Frequency')
plt.grid(True, linestyle='--', alpha=0.6)

# KDE Plot
plt.subplot(1, 2, 2)
sns.kdeplot(results_df['absolute_percentage_error'], fill=True, color='lightcoral')
plt.title('Kernel Density Estimate of Prediction Error')
plt.xlabel('Error (Actual - Mean)')
plt.ylabel('Density')
plt.grid(True, linestyle='--', alpha=0.6)

plt.tight_layout()
plt.show()

#%%
# 2. Plot the relationship between the absolute value of the error and the amount of periods since last refit
# Convert periods_since_refit to days (assuming hourly data, 24 periods per day)
results_df['days_since_refit'] = results_df['periods_since_refit'] / 24
results_df['days_since_refit'] = results_df['days_since_refit'].astype(int)
plt.figure(figsize=(12, 7))
sns.boxplot(
    data=results_df,
    x='days_since_refit',
    y='absolute_percentage_error',
)

plt.title('Percentage Absolute Error vs. Days Since Last Model Refit')
plt.xlabel('Days Since Last Refit')
plt.ylabel('% Absolute Error |Actual - Mean| / Actual')
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend(title='Days Since Refit')
plt.tight_layout()
plt.show()
#%%
# 3. Plot the error through time
plt.figure(figsize=(15, 7))
plt.plot(results_df.index, results_df['error'], label='Prediction Error', color='darkorange', alpha=0.8)
plt.axhline(0, color='gray', linestyle='--', linewidth=1) # Zero error line
plt.title('Prediction Error Through Time')
plt.xlabel('Date')
plt.ylabel('Error (Actual - Mean)')
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend()
plt.tight_layout()
plt.show()

#%%
# Optional: Plot absolute percentage error through time
plt.figure(figsize=(15, 7))
plt.plot(results_df.index, results_df['absolute_percentage_error'], label='Absolute Percentage Error', color='purple', alpha=0.8)
plt.title('Absolute Percentage Error Through Time')
plt.xlabel('Date')
plt.ylabel('Absolute Percentage Error (%)')
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend()
plt.tight_layout()
plt.show()


#%%
# 4. Compare MAPE of mean vs percentile predictions
# Configuration
RESULTS_FILE = 'predictions_trial_1.csv'
# Load the backtest results
try:
    results_df = pd.read_csv(RESULTS_FILE, index_col='timestamp', parse_dates=True)
    print(f"Successfully loaded data from {RESULTS_FILE}. Shape: {results_df.shape}")
except FileNotFoundError:
    print(f"Error: The file '{RESULTS_FILE}' was not found. Please ensure it has been generated by 'run_regression_backtest()'.")
    exit()

# Calculate error metrics
results_df['error'] = results_df['actual_close'] - results_df['mean']
results_df['absolute_error'] = np.abs(results_df['error'])
results_df['percentage_error'] = (results_df['error'] / results_df['actual_close']) * 100
results_df['absolute_percentage_error'] = np.abs(results_df['percentage_error'])

prediction_cols = ['mean'] + [f'{q:.1f}' for q in np.arange(0.1, 1.0, 0.1)]
# Filter for columns that actually exist in the dataframe
prediction_cols = [col for col in prediction_cols if col in results_df.columns]

if prediction_cols:
    mape_scores = {}
    for col in prediction_cols:
        # Calculate Absolute Percentage Error for the column
        ape = np.abs((results_df['actual_close'] - results_df[col]) / results_df['actual_close']) * 100
        mape_scores[col] = ape.mean()

    # Create a DataFrame for plotting
    mape_df = pd.DataFrame(list(mape_scores.items()), columns=['Prediction', 'MAPE']).sort_values('Prediction')

    # Plotting
    plt.figure(figsize=(12, 7))
    sns.barplot(data=mape_df, x='Prediction', y='MAPE', palette='viridis')
    plt.title('Mean Absolute Percentage Error (MAPE) for Mean and Percentile Predictions', fontsize=16)
    plt.xlabel('Prediction Type (Mean / Percentile)', fontsize=12)
    plt.ylabel('Mean Absolute Percentage Error (%)', fontsize=12)
    plt.xticks(rotation=0)
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.show()

#%%
# 5. Plot actual vs. predicted values over time
plt.figure(figsize=(15, 8))

# Plot actual closing price
plt.plot(results_df.index, results_df['actual_close'], label='Actual Close', color='blue', linewidth=1.5)

# Plot mean prediction
plt.plot(results_df.index, results_df['mean'], label='Mean Prediction', color='orange', linestyle='--', linewidth=1.5)

# Plot prediction intervals (e.g., 10th to 90th percentile)
if '0.1' in results_df.columns and '0.9' in results_df.columns:
    plt.fill_between(results_df.index, results_df['0.1'], results_df['0.9'], color='orange', alpha=0.2, label='10-90th Percentile Range')

plt.title('Actual vs. Predicted Close Price Over Time', fontsize=16)
plt.xlabel('Date', fontsize=12)
plt.ylabel('Price', fontsize=12)
plt.legend()
plt.grid(True, linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()
