#%%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Configuration
RESULTS_FILE = 'regression_backtest_results.csv'
#AI the columns are timestamp,actual_close,mean,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9 adapt the code AI!  
# Load the backtest results
try:
    results_df = pd.read_csv(RESULTS_FILE, index_col='timestamp', parse_dates=True)
    print(f"Successfully loaded data from {RESULTS_FILE}. Shape: {results_df.shape}")
except FileNotFoundError:
    print(f"Error: The file '{RESULTS_FILE}' was not found. Please ensure it has been generated by 'run_regression_backtest()'.")
    exit()

# Calculate error metrics
results_df['error'] = results_df['actual_close'] - results_df['predicted_close']
results_df['absolute_error'] = np.abs(results_df['error'])
results_df['percentage_error'] = (results_df['error'] / results_df['actual_close']) * 100
results_df['absolute_percentage_error'] = np.abs(results_df['percentage_error'])

print("\nError Statistics:")
print(results_df[['error', 'absolute_error', 'percentage_error', 'absolute_percentage_error']].describe())

#%%
# --- Comparison of multiple backtest results ---


RESULTS_FILES = [
    'regression_backtest_results.csv',
    'regression_backtest_results_7d.csv',
    'regression_backtest_results_14d.csv',
    'regression_backtest_results_365d.csv'
]

all_stats_list = []
summary_metrics = []
for file in RESULTS_FILES:
    try:
        df = pd.read_csv(file, index_col='timestamp', parse_dates=True)
        # Create a more readable model name from the filename
        model_name = file.replace('_backtest_results.csv', '').replace('regression_', '')

        # Calculate error metrics
        df['error'] = df['actual_close'] - df['predicted_close']
        df['absolute_error'] = np.abs(df['error'])
        df['percentage_error'] = (df['error'] / df['actual_close']) * 100
        df['absolute_percentage_error'] = np.abs(df['percentage_error'])

        # Get summary statistics
        stats = df[['error', 'absolute_error', 'percentage_error', 'absolute_percentage_error']].describe()
        stats['model'] = model_name
        all_stats_list.append(stats)

        # Calculate summary metrics like MAE, RMSE, MASE
        mae = df['absolute_error'].mean()
        mape = df['absolute_percentage_error'].mean()
        rmse = np.sqrt((df['error']**2).mean())
        mae_naive = np.abs(df['actual_close'].diff()).mean()
        mase = mae / mae_naive if mae_naive != 0 else np.inf

        summary_metrics.append({
            'model': model_name,
            'MAE': mae,
            'MAPE': mape,
            'RMSE': rmse,
            'MASE': mase,
        })
        
    except FileNotFoundError:
        print(f"\nWarning: The file '{file}' was not found. Skipping comparison for this file.")

if all_stats_list:
    # Prepare dataframe for plotting
    comparison_df = pd.concat(all_stats_list)
    comparison_df = comparison_df.reset_index().rename(columns={'index': 'statistic'})
    comparison_df = comparison_df.set_index(['model', 'statistic'])

    print("\n--- Model Comparison Statistics ---")
    print(comparison_df)

    # Plotting
    metrics_to_plot = ['error', 'absolute_error', 'percentage_error', 'absolute_percentage_error']
    stats_to_plot = ['mean', 'std', 'min', '25%', '50%', '75%', 'max']

    for metric in metrics_to_plot:
        # Select data for the current metric and stats to plot
        # Using .loc to avoid potential KeyError if a stat is missing, though describe() is consistent
        plot_data = comparison_df[metric].unstack(level='model').loc[stats_to_plot]

        # Plotting the comparison
        plot_data.plot(kind='bar', figsize=(14, 8), rot=0, width=0.8)
        plt.title(f'Comparison of Statistics for: {metric.replace("_", " ").title()}', fontsize=16)
        plt.xlabel('Statistic')
        plt.ylabel('Value')
        plt.grid(axis='y', linestyle='--', alpha=0.7)
        plt.legend(title='Model')
        plt.tight_layout()
        plt.show()

if summary_metrics:
    summary_df = pd.DataFrame(summary_metrics).set_index('model')
    print("\n--- Model Comparison (Summary Metrics) ---")
    print(summary_df)

    # Plotting
    summary_df.plot(kind='bar', figsize=(12, 8), subplots=True, layout=(2, 2), sharey=False, rot=45)
    plt.suptitle('Comparison of Summary Error Metrics', fontsize=16)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()
#%%
# --- Detailed analysis of primary file continues below ---
print(f"\n--- Detailed Analysis for: {RESULTS_FILE} ---")

# 1. Analyze the distribution of the error
plt.figure(figsize=(15, 6))

# Histogram
plt.subplot(1, 2, 1)
sns.histplot(results_df['error'], kde=True, bins=50, color='skyblue')
plt.title('Distribution of Prediction Error')
plt.xlabel('Error (Actual - Predicted)')
plt.ylabel('Frequency')
plt.grid(True, linestyle='--', alpha=0.6)

# KDE Plot
plt.subplot(1, 2, 2)
sns.kdeplot(results_df['error'], fill=True, color='lightcoral')
plt.title('Kernel Density Estimate of Prediction Error')
plt.xlabel('Error (Actual - Predicted)')
plt.ylabel('Density')
plt.grid(True, linestyle='--', alpha=0.6)

plt.tight_layout()
plt.show()

#%%
plt.figure(figsize=(15, 6))
# Histogram
plt.subplot(1, 2, 1)
sns.histplot(results_df['percentage_error'], kde=True, bins=50, color='skyblue')
plt.title('Distribution of Prediction Error')
plt.xlabel('Error (Actual - Predicted)')
plt.ylabel('Frequency')
plt.grid(True, linestyle='--', alpha=0.6)

# KDE Plot
plt.subplot(1, 2, 2)
sns.kdeplot(results_df['percentage_error'], fill=True, color='lightcoral')
plt.title('Kernel Density Estimate of Prediction Error')
plt.xlabel('Error (Actual - Predicted)')
plt.ylabel('Density')
plt.grid(True, linestyle='--', alpha=0.6)

plt.tight_layout()
plt.show()

#%%
plt.figure(figsize=(15, 6))
# Histogram
plt.subplot(1, 2, 1)
sns.histplot(results_df['absolute_percentage_error'], kde=True, bins=50, color='skyblue')
plt.title('Distribution of Prediction Error')
plt.xlabel('Error (Actual - Predicted)')
plt.ylabel('Frequency')
plt.grid(True, linestyle='--', alpha=0.6)

# KDE Plot
plt.subplot(1, 2, 2)
sns.kdeplot(results_df['absolute_percentage_error'], fill=True, color='lightcoral')
plt.title('Kernel Density Estimate of Prediction Error')
plt.xlabel('Error (Actual - Predicted)')
plt.ylabel('Density')
plt.grid(True, linestyle='--', alpha=0.6)

plt.tight_layout()
plt.show()

#%%
# 2. Plot the relationship between the absolute value of the error and the amount of periods since last refit
# Convert periods_since_refit to days (assuming hourly data, 24 periods per day)
results_df['days_since_refit'] = results_df['periods_since_refit'] / 24
results_df['days_since_refit'] = results_df['days_since_refit'].astype(int)
plt.figure(figsize=(12, 7))
sns.boxplot(
    data=results_df,
    x='days_since_refit',
    y='absolute_percentage_error',
)

plt.title('Percentage Absolute Error vs. Days Since Last Model Refit')
plt.xlabel('Days Since Last Refit')
plt.ylabel('% Absolute Error |Actual - Predicted| / Actual')
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend(title='Days Since Refit')
plt.tight_layout()
plt.show()
#%%
# 3. Plot the error through time
plt.figure(figsize=(15, 7))
plt.plot(results_df.index, results_df['error'], label='Prediction Error', color='darkorange', alpha=0.8)
plt.axhline(0, color='gray', linestyle='--', linewidth=1) # Zero error line
plt.title('Prediction Error Through Time')
plt.xlabel('Date')
plt.ylabel('Error (Actual - Predicted)')
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend()
plt.tight_layout()
plt.show()

#%%
# Optional: Plot absolute percentage error through time
plt.figure(figsize=(15, 7))
plt.plot(results_df.index, results_df['absolute_percentage_error'], label='Absolute Percentage Error', color='purple', alpha=0.8)
plt.title('Absolute Percentage Error Through Time')
plt.xlabel('Date')
plt.ylabel('Absolute Percentage Error (%)')
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend()
plt.tight_layout()
plt.show()

